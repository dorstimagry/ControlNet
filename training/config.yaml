env:
  # Action space bounds (dimensionless, -1 to 1)
  action_high: 1.0          # Maximum action value (full throttle/brake)
  action_low: -1.0          # Minimum action value (full brake)

  # Reward function weights (dimensionless, affect reward scaling)
  accel_filter_alpha: 0.2   # Exponential smoothing factor for acceleration (0=no smoothing, 1=instant)
  action_weight: 0.01       # Penalty weight for action magnitude (reduces aggressive control)
  brake_weight: 0.0         # Penalty weight for brake usage (set to 0 to disable)
  horizon_penalty_weight: 0.0  # Weight for future horizon tracking penalties
  horizon_penalty_decay: 0.95  # Exponential decay factor for future penalties (0.9-1.0)
  jerk_weight: 0.1          # Penalty weight for acceleration changes (smoothness)
  smooth_action_weight: 0.25 # Penalty weight for action changes (stability)
  track_weight: 1.0         # Weight for speed tracking error in reward
  voltage_weight: 0.0       # Penalty weight for motor voltage (set to 0 to disable)

  # Simulation parameters
  dt: 0.1                   # Policy step time (seconds) - controls control frequency
  max_episode_steps: 2048   # Maximum steps per episode (affects episode length)
  max_position: 5000.0      # Maximum position before episode termination (meters)
  max_speed: 25.0           # Maximum allowed speed (m/s) - affects reward clipping
  plant_substeps: 5         # Physics simulation substeps per policy step (stability)
  preview_horizon_s: 3.0    # How far ahead the agent can see reference speeds (seconds)
  use_extended_plant: true  # Use detailed vehicle dynamics model (true) or simple (false)
output:
  dir: training/checkpoints     # Directory to save model checkpoints
  save_latest: true             # Whether to save the latest checkpoint during training
reference_dataset: null         # Path to reference dataset (null = use generator)
seed: 42                        # Random seed for reproducibility
training:
  # SAC algorithm hyperparameters
  gamma: 0.95                # Discount factor for future rewards (0.9-0.99)
  learning_rate: 0.0003      # Learning rate for policy/critic updates
  tau: 0.01                  # Soft update coefficient for target networks (0.001-0.01)
  target_entropy_scale: 0.98 # Target entropy scaling for automatic temperature tuning

  # Training schedule
  num_train_timesteps: 1000000  # Total training steps (environment interactions)
  warmup_steps: 10000         # Steps before training starts (random exploration)
  batch_size: 256             # Minibatch size for training updates
  replay_size: 1000000        # Maximum replay buffer size (steps)

  # Logging and checkpointing
  log_interval: 1000          # Steps between logging training metrics
  checkpoint_interval: 10000  # Steps between saving model checkpoints
  max_grad_norm: 5.0          # Maximum gradient norm for clipping (stability)

  # Evaluation during training
  eval_interval: 5000         # Steps between evaluation runs during training
  eval_episodes: 5            # Number of episodes per evaluation run

  # Action planning (for MPC-style policies, if used)
  action_horizon_steps: 10    # Planning horizon for action sequences (steps)
generator:
  # Low-Pass Filter (LPF) parameters for smoothing target speed profiles
  freq_cutoff: 0.8         # LPF cutoff frequency (Hz) - higher = more responsive but less smooth
  zeta: 3.0                # Damping ratio - higher = more stable but slower response
  dt: 0.02                 # LPF internal timestep (seconds) - affects smoothness resolution
  jerk_max: 4.0            # Maximum allowed jerk (m/s³) - limits acceleration changes

  # Event-driven target generation parameters
  p_change: 0.03           # Probability of changing target speed per step (0-1)
  p_zero_stop: 0.08        # Probability of sampling full stops vs. speed targets (0-1)
  v_min: 0.0               # Minimum speed for random targets (m/s)
  v_max_sample: 25.0       # Maximum speed for random targets (m/s)
  t_min: 2.0               # Minimum time to reach new target (seconds)
  t_max: 12.0              # Maximum time to reach new target (seconds)
  stop_hold_min: 1.0       # Minimum dwell time for full stops (seconds)
  stop_hold_max: 5.0       # Maximum dwell time for full stops (seconds)

  # Stochastic acceleration/jerk sampling parameters
  p_change_acc: 0.01       # Probability of changing acceleration scale per step (0-1)
  p_change_jerk: 0.01      # Probability of changing jerk scale per step (0-1)

  # Road grade generation parameters (Ornstein-Uhlenbeck process)
  ds: 1.0                  # Spatial grid step size (meters) - resolution of grade map
  l_corr: 150.0             # Spatial correlation length (meters) - road segment smoothness
  sigma_g_stat: 0.025      # Grade standard deviation (radians) - ~0.29° typical variation
  g_min: -0.08             # Minimum grade angle (radians) - ~ -4.6° max downhill
  g_max: 0.08              # Maximum grade angle (radians) - ~ +4.6° max uphill
